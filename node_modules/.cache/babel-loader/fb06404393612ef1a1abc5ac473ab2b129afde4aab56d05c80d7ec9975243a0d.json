{"ast":null,"code":"import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from \"@google/generative-ai\";\nconst apiKey = process.env.GEMINI_API_KEY || \"AIzaSyAnGJBRlC39Hfl52yxDH3DTFCySZUJgZ4Y\";\nconst genAI = new GoogleGenerativeAI(apiKey);\nconst model = genAI.getGenerativeModel({\n  model: \"gemini-1.5-flash\"\n});\nconst generationConfig = {\n  temperature: 1,\n  topP: 0.95,\n  topK: 64,\n  maxOutputTokens: 8192,\n  responseMimeType: \"text/plain\"\n};\nasync function run(prompt) {\n  const chatSession = model.startChat({\n    generationConfig,\n    history: []\n  });\n  try {\n    const result = await chatSession.sendMessage(prompt);\n    console.log(\"API Response:\", result.response.text); // Log the API response\n    return result.response.text;\n  } catch (error) {\n    console.error(\"Error during API call:\", error);\n    throw new Error(\"Failed to fetch response from the Generative AI API\");\n  }\n}\nexport default run;","map":{"version":3,"names":["GoogleGenerativeAI","HarmCategory","HarmBlockThreshold","apiKey","process","env","GEMINI_API_KEY","genAI","model","getGenerativeModel","generationConfig","temperature","topP","topK","maxOutputTokens","responseMimeType","run","prompt","chatSession","startChat","history","result","sendMessage","console","log","response","text","error","Error"],"sources":["C:/Users/Mohana/Desktop/geminiai/geminiai/src/Config/gemini.js"],"sourcesContent":["\r\n\r\n\r\n\r\nimport {\r\n    GoogleGenerativeAI,\r\n    HarmCategory,\r\n    HarmBlockThreshold,\r\n  } from \"@google/generative-ai\";\r\n  \r\n  const apiKey = process.env.GEMINI_API_KEY || \"AIzaSyAnGJBRlC39Hfl52yxDH3DTFCySZUJgZ4Y\";\r\n  const genAI = new GoogleGenerativeAI(apiKey);\r\n  \r\n  const model = genAI.getGenerativeModel({\r\n    model: \"gemini-1.5-flash\",\r\n  });\r\n  \r\n  const generationConfig = {\r\n    temperature: 1,\r\n    topP: 0.95,\r\n    topK: 64,\r\n    maxOutputTokens: 8192,\r\n    responseMimeType: \"text/plain\",\r\n  };\r\n  \r\n  async function run(prompt) {\r\n    const chatSession = model.startChat({\r\n      generationConfig,\r\n      history: [],\r\n    });\r\n  \r\n    try {\r\n      const result = await chatSession.sendMessage(prompt);\r\n      console.log(\"API Response:\", result.response.text); // Log the API response\r\n      return result.response.text;\r\n    } catch (error) {\r\n      console.error(\"Error during API call:\", error);\r\n      throw new Error(\"Failed to fetch response from the Generative AI API\");\r\n    }\r\n  }\r\n  \r\n  export default run;\r\n  "],"mappings":"AAIA,SACIA,kBAAkB,EAClBC,YAAY,EACZC,kBAAkB,QACb,uBAAuB;AAE9B,MAAMC,MAAM,GAAGC,OAAO,CAACC,GAAG,CAACC,cAAc,IAAI,yCAAyC;AACtF,MAAMC,KAAK,GAAG,IAAIP,kBAAkB,CAACG,MAAM,CAAC;AAE5C,MAAMK,KAAK,GAAGD,KAAK,CAACE,kBAAkB,CAAC;EACrCD,KAAK,EAAE;AACT,CAAC,CAAC;AAEF,MAAME,gBAAgB,GAAG;EACvBC,WAAW,EAAE,CAAC;EACdC,IAAI,EAAE,IAAI;EACVC,IAAI,EAAE,EAAE;EACRC,eAAe,EAAE,IAAI;EACrBC,gBAAgB,EAAE;AACpB,CAAC;AAED,eAAeC,GAAGA,CAACC,MAAM,EAAE;EACzB,MAAMC,WAAW,GAAGV,KAAK,CAACW,SAAS,CAAC;IAClCT,gBAAgB;IAChBU,OAAO,EAAE;EACX,CAAC,CAAC;EAEF,IAAI;IACF,MAAMC,MAAM,GAAG,MAAMH,WAAW,CAACI,WAAW,CAACL,MAAM,CAAC;IACpDM,OAAO,CAACC,GAAG,CAAC,eAAe,EAAEH,MAAM,CAACI,QAAQ,CAACC,IAAI,CAAC,CAAC,CAAC;IACpD,OAAOL,MAAM,CAACI,QAAQ,CAACC,IAAI;EAC7B,CAAC,CAAC,OAAOC,KAAK,EAAE;IACdJ,OAAO,CAACI,KAAK,CAAC,wBAAwB,EAAEA,KAAK,CAAC;IAC9C,MAAM,IAAIC,KAAK,CAAC,qDAAqD,CAAC;EACxE;AACF;AAEA,eAAeZ,GAAG","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}